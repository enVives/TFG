{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMGfae6ShbpOytRSVtjAf8v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enVives/TFG/blob/main/FeatureExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracció dels vectors de característiques\n",
        "\n",
        "Aquest fitxer està pensat per ser utilitzat després dels entrenaments fets\n",
        "en el fitxer de EvaluacioArquitectures. Utilitzarem els pesos del model elegit per a guardar els vectors de característiques de les imatges pertanyents al conjunt d'entrenament. Aquests vectors es guardaran dins una carpeta anomenada 'Features' que es situarà dins la mateixa carpeta dels pesos del model.\n",
        "\n",
        "D'aquesta manera tindrem una carpeta per a cada model. De cada model hi tindrem una carpeta numerada per a cada execució i, dins cada una d'aquestes, hi guardarem els vectors de característiques a la carpeta 'Features'."
      ],
      "metadata": {
        "id": "7Hw6WKkOlbPr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaQglPekfOwo",
        "outputId": "8f184555-378b-4af0-f2da-ed6aa43b0a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import pylab as pl\n",
        "import json\n",
        "\n",
        "from glob import glob\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms,models\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output,display\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from google.colab import drive\n",
        "\n",
        "DOWNLOAD = False\n",
        "\n",
        "\n",
        "#Name of the trained models\n",
        "MODELS = {\n",
        "    0: 'AlexnetBinary',\n",
        "    1: 'Resnet152Binary',\n",
        "    2: 'Inceptionv3Binary',\n",
        "    3: 'EfficientNetB1Binary',\n",
        "    4: 'AlexnetMulticlass',\n",
        "    5: 'Resnet152Multiclass',\n",
        "    6: 'Inceptionv3Multiclass',\n",
        "    7: 'EfficientnetB1Multiclass',\n",
        "}\n",
        "\n",
        "\n",
        "MODEL = MODELS[6] #triam un model\n",
        "RUN = 9 #triam el nombre de l'execució a la que s'han guardat els pesos (el nombre de la carpeta)\n",
        "\n",
        "\n",
        "WEIGHTS_PATH = \"/content/drive/MyDrive/Runs/Ham10000-\"+MODEL+\"/\"+str(RUN)+\"/\" #camí dels pesos\n",
        "TRAINING_INFO = \"/content/drive/MyDrive/Runs/Ham10000-\"+MODEL+\"/\"+str(RUN)+\"/training_info.json\" #camí de les dades guardades de l'entrenament\n",
        "#/content/drive/MyDrive/Runs/Ham10000Resnet152Multiclass/5/pesos_resnet152multiclass_5.pt\n",
        "pt_file = glob(WEIGHTS_PATH+\"*.pt\") #fitxer de pesos\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "if DOWNLOAD:\n",
        "  !rm -rf /content/sample_data/*\n",
        "\n",
        "  !kaggle datasets download -d \"kmader/skin-cancer-mnist-ham10000\"\n",
        "\n",
        "  !unzip -o skin-cancer-mnist-ham10000.zip -d /content/sample_data/\n",
        "\n",
        "  !kaggle datasets download -d \"tschandl/ham10000-lesion-segmentations\"\n",
        "\n",
        "  !unzip -o ham10000-lesion-segmentations.zip -d /content/sample_data/\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuació cercam quins paràmetres hem utilitzat en el conjunt d'entrenament per a replicar el mateix escenari."
      ],
      "metadata": {
        "id": "qfM0uTs4nQpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(TRAINING_INFO, 'r') as f:\n",
        "    training_info = json.load(f)\n",
        "\n",
        "SEGMENTATION = training_info['SEGMENTATION'] #utilitzam els mateixos hiperparàmetres que a l'entrenament\n",
        "CROPPING = training_info['CROPPING']\n",
        "SIZE = training_info['img_size']"
      ],
      "metadata": {
        "id": "fcZcC_vKWuxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !mv /content/kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "zFYkx-KkqI8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadates = pd.read_csv('/content/sample_data/HAM10000_metadata.csv')\n",
        "metadates = metadates.sort_values(by='image_id')"
      ],
      "metadata": {
        "id": "GqqiDKpHwCHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Formes(Dataset):\n",
        "\n",
        "  def __init__(self, images, labels, transform):\n",
        "        super().__init__()\n",
        "        self.paths = images\n",
        "        self.labels = labels\n",
        "        self.len = len(self.paths)\n",
        "        self.transform = transform\n",
        "        self.greyscale = False\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "  def __setmasks__(self,masks):\n",
        "      self.masks = masks\n",
        "\n",
        "  def __getmask__(self,i):\n",
        "      return self.masks[i]\n",
        "\n",
        "  def __lenmasks__(self):\n",
        "    return len(self.masks)\n",
        "\n",
        "  def __addlabel__(self,label):\n",
        "    self.labels = np.append(self.labels,label)\n",
        "\n",
        "  def __addPath__(self,path):\n",
        "    self.paths = np.append(self.paths,path)\n",
        "\n",
        "  def __getdist__(self):\n",
        "    return pd.Series(self.labels).value_counts()\n",
        "\n",
        "  def __getlabels__(self):\n",
        "    classes = [[],[],[],[],[],[],[]]\n",
        "    for i in range(len(self.labels)):\n",
        "      classes[self.labels[i]].append(i) #afegim l'index\n",
        "    return classes\n",
        "\n",
        "  def __setgreyscale__(self,mode):\n",
        "    self.greyscale = mode\n",
        "\n",
        "  def __getpath__(self,index):\n",
        "    return self.paths[index]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      path = self.paths[index]\n",
        "      label = self.labels[index]\n",
        "\n",
        "      image = cv2.imread(path, cv2.IMREAD_GRAYSCALE if self.greyscale else cv2.IMREAD_COLOR)\n",
        "\n",
        "      if SEGMENTATION:\n",
        "        mask = cv2.imread(self.masks[index], cv2.IMREAD_GRAYSCALE)\n",
        "        _, mask_binaria = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
        "        image = cv2.bitwise_and(image, image, mask=mask_binaria)\n",
        "\n",
        "        non_black_pixels = np.where(image > 0)\n",
        "\n",
        "        x_min, x_max = np.min(non_black_pixels[1]), np.max(non_black_pixels[1])\n",
        "        y_min, y_max = np.min(non_black_pixels[0]), np.max(non_black_pixels[0])\n",
        "\n",
        "        marge = 10\n",
        "        if ((x_min - marge) >= 0):\n",
        "            x_min = x_min - marge\n",
        "        if ((x_max + marge) <= SIZE):\n",
        "            x_max = x_max + marge\n",
        "\n",
        "        if CROPPING:\n",
        "          image = image[y_min:y_max, x_min:x_max]\n",
        "\n",
        "        image = cv2.resize(image, (SIZE, SIZE), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "\n",
        "      if self.greyscale:\n",
        "        image = Image.fromarray(image, mode=\"L\")\n",
        "      else:\n",
        "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "      image = self.transform(image)\n",
        "\n",
        "      return image, label"
      ],
      "metadata": {
        "id": "FhUvwbXdwGyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sets(transformation_training,transformation_default,training_dist,valitation_dist,testing_dist):\n",
        "\n",
        "  illnes_dictionary = {\n",
        "      'nv': 'Melanocytic nevi',\n",
        "      'mel': 'Melanoma',\n",
        "      'bkl': 'Benign keratosis-like lesions ',\n",
        "      'bcc': 'Basal cell carcinoma',\n",
        "      'akiec': 'Actinic keratoses and intraepithelial carcinoma / Bowens disease',\n",
        "      'vasc': 'Vascular lesions',\n",
        "      'df': 'Dermatofibroma'\n",
        "  }\n",
        "\n",
        "  img_files_1 = sorted(glob('/content/sample_data/HAM10000_images_part_1/*'))\n",
        "  img_files_2 = sorted(glob('/content/sample_data/HAM10000_images_part_2/*'))\n",
        "  img_files = img_files_1 + img_files_2\n",
        "\n",
        "  img_files = np.array(img_files)\n",
        "\n",
        "  mask_files = sorted(glob('/content/sample_data/HAM10000_segmentations_lesion_tschandl/*'))\n",
        "\n",
        "  imgid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in img_files}\n",
        "\n",
        "  mask_path_dict = {os.path.splitext(os.path.basename(x))[0].replace('_segmentation', ''): x for x in mask_files}\n",
        "\n",
        "  metadates['path'] = metadates['image_id'].map(imgid_path_dict.get)\n",
        "  metadates['mask_path'] = metadates['image_id'].map(mask_path_dict.get)\n",
        "\n",
        "  metadates['illness'] = metadates['dx'].map(illnes_dictionary.get)\n",
        "  metadates['illness_code'] = metadates['dx'].map({'nv': 0, 'mel': 1, 'bkl': 2, 'bcc': 3, 'akiec': 4, 'vasc': 5, 'df': 6})\n",
        "\n",
        "  #Aquest illness_code s'utilitzarà com a label de la enfermetat\n",
        "\n",
        "  img_number = len(img_files)\n",
        "\n",
        "  X = metadates.drop('illness_code',axis= 1)\n",
        "  y = metadates['illness_code']\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testing_dist, random_state=42, stratify=y)\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=valitation_dist/(training_dist+valitation_dist), random_state=42, stratify=y_train)\n",
        "\n",
        "  train_data = Formes(X_train['path'].to_numpy(),y_train.to_numpy(),transformation_training)\n",
        "  test_data = Formes(X_test['path'].to_numpy(),y_test.to_numpy(),transformation_default)\n",
        "  validation_data = Formes(X_val['path'].to_numpy(),y_val.to_numpy(),transformation_default)\n",
        "\n",
        "  if SEGMENTATION:\n",
        "\n",
        "    train_data.__setmasks__(X_train['mask_path'].to_numpy())\n",
        "    test_data.__setmasks__(X_test['mask_path'].to_numpy())\n",
        "    validation_data.__setmasks__(X_val['mask_path'].to_numpy())\n",
        "\n",
        "  return train_data, validation_data,test_data\n",
        "\n"
      ],
      "metadata": {
        "id": "sPS_7_cjwK4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genera_embeddings(model,dataset,activation,features_file):\n",
        "  total = dataset.__len__()\n",
        "  np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "  data = [[\"imatge\",\"cnnoutput\",\"fcoutput\",\"path\",\"label\"]]\n",
        "\n",
        "  f = WEIGHTS_PATH+features_file\n",
        "\n",
        "  # Camps que guardarem, FC Input és el vector de característiques\n",
        "  headers = [\"Image Name\", \"FC Input\", \"Image Path\", \"Label\"]\n",
        "\n",
        "  with open(f, mode=\"w\", newline=\"\") as file:\n",
        "      writer = csv.writer(file, delimiter=',')\n",
        "      writer.writerow(headers)\n",
        "\n",
        "  for i in range(total):\n",
        "      imatge, label = dataset.__getitem__(i)\n",
        "      imatge = imatge.to(device)\n",
        "      imatge = imatge.unsqueeze(0)\n",
        "\n",
        "      img_path = dataset.__getpath__(i)\n",
        "      img_name = img_path.split('/')[-1]\n",
        "\n",
        "      with torch.no_grad():\n",
        "        output = model(imatge)\n",
        "\n",
        "      #cnnoutput = activation['layer4'].flatten()\n",
        "      fcoutput = activation['lastlayer'].flatten()\n",
        "\n",
        "      data_row = [img_name, fcoutput.cpu().detach().tolist(), img_path, str(label)]\n",
        "\n",
        "      with open(f, mode=\"a\", newline=\"\") as file:\n",
        "          writer = csv.writer(file, delimiter=',')\n",
        "          writer.writerow(data_row)  # Write row immediately\n",
        "\n",
        "      # if i == 1:\n",
        "      #   break\n",
        "\n",
        "  print(\"Txt file writing completed.\")"
      ],
      "metadata": {
        "id": "v1QfLKvG3MsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = torch.tensor([194.57463374, 139.13953272, 145.36132088]) /255 #rgb\n",
        "std = torch.tensor([35.92275236, 38.90347617, 43.33101831]) / 255\n",
        "\n",
        "TRAINING = 0.80\n",
        "VALIDATION = 0.10\n",
        "TESTING = 0.10\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((SIZE,SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = mean, std=std)\n",
        "])\n",
        "\n",
        "transform_training = transforms.Compose([\n",
        "    transforms.Resize((SIZE,SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.3),\n",
        "    transforms.RandomVerticalFlip(p=0.3),\n",
        "    transforms.RandomRotation(degrees=20),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = mean, std=std),\n",
        "])\n",
        "\n",
        "train_data,validation_data,test_data = load_sets(transform_training,transform,TRAINING,VALIDATION,TESTING)\n",
        "\n",
        "\n",
        "#Aquí posam el millor model triat\n",
        "inceptionv3 = models.inception_v3()\n",
        "num_features = inceptionv3.fc.in_features\n",
        "inceptionv3.fc = nn.Linear(in_features=num_features, out_features=7)\n",
        "\n",
        "#Li passam el fitxer de pesos\n",
        "inceptionv3.load_state_dict(torch.load(pt_file[0], weights_only=True))\n",
        "\n",
        "#definim la funció per obtenir els vectors de característiques\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = input[0].detach()\n",
        "    return hook\n",
        "\n",
        "model = inceptionv3\n",
        "model.fc.register_forward_hook(get_activation('lastlayer'))\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "os.makedirs(WEIGHTS_PATH+'Features',exist_ok=True)\n",
        "\n",
        "model.eval()\n",
        "genera_embeddings(model,train_data,activation,\"Features/training_features.txt\")\n",
        "genera_embeddings(model,test_data,activation,\"Features/testing_features.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9myxhQ40wUTF",
        "outputId": "f7306c6f-bdd3-4ad4-9160-270515e5a23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Txt file writing completed.\n",
            "Txt file writing completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "MQJbxV-TgNcy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}