{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOxxSnLNvttXSTbK0kB4ZdP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enVives/TFG/blob/main/FeatureDistances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBBrozgPtidt",
        "outputId": "c9bf387d-597c-4aba-e0c5-b713a52d4927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import pylab as pl\n",
        "import json\n",
        "\n",
        "from glob import glob\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms,models\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output,display\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "DOWNLOAD = False\n",
        "CLAHE = False\n",
        "SEGMENTATION = True\n",
        "# ORIGEN = '/content/drive/MyDrive/HAM10000/skin-cancer-mnist-ham10000/'\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "if DOWNLOAD:\n",
        "  !rm -rf /content/sample_data/*\n",
        "\n",
        "  !kaggle datasets download -d \"kmader/skin-cancer-mnist-ham10000\"\n",
        "\n",
        "  !unzip -o skin-cancer-mnist-ham10000.zip -d /content/sample_data/\n",
        "\n",
        "  !kaggle datasets download -d \"tschandl/ham10000-lesion-segmentations\"\n",
        "\n",
        "  !unzip -o ham10000-lesion-segmentations.zip -d /content/sample_data/\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !mv /content/kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "aQgSftipgVUt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadates = pd.read_csv('/content/sample_data/HAM10000_metadata.csv')\n",
        "metadates = metadates.sort_values(by='image_id')"
      ],
      "metadata": {
        "id": "gx-XW9ApuJY7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Formes(Dataset):\n",
        "  #Classe on gestionarem les imatges dels fitxers\n",
        "  dict_illnesses = {0 : 'nv', 1 : 'mel', 2 : 'bkl', 3 : 'bcc', 4 : 'akiec', 5 : 'vasc', 6 : 'df'}\n",
        "\n",
        "  def __init__(self, images, labels, transform):\n",
        "        super().__init__()\n",
        "        self.paths = images\n",
        "        self.labels = labels\n",
        "        self.len = len(self.paths)\n",
        "        self.transform = transform\n",
        "        #Per defecte pens que el color pot extreure característiques importants, per tant en primer lloc\n",
        "        #entrenarem les imatges de color\n",
        "        self.greyscale = False\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "  def __setmasks__(self,masks):\n",
        "      self.masks = masks\n",
        "\n",
        "  def __getmask__(self,i):\n",
        "      return self.masks[i]\n",
        "\n",
        "  def __lenmasks__(self):\n",
        "    return len(self.masks)\n",
        "\n",
        "  def __addlabel__(self,label):\n",
        "    self.labels = np.append(self.labels,label)\n",
        "\n",
        "  def __addPath__(self,path):\n",
        "    self.paths = np.append(self.paths,path)\n",
        "\n",
        "  def __getdist__(self):\n",
        "    return pd.Series(self.labels).value_counts()\n",
        "\n",
        "  def __getlabel__(self,i):\n",
        "    return self.labels[i]\n",
        "\n",
        "  def __getlabels__(self):\n",
        "    classes = [[],[],[],[],[],[],[]]\n",
        "    for i in range(len(self.labels)):\n",
        "      classes[self.labels[i]].append(i) #afegim l'index\n",
        "    return classes\n",
        "\n",
        "  def __redistribute__(self,percentages):\n",
        "    #percentages: [15,15,10,10,5,5] percentatges que volem pujar de la resta de classes llevat de nv\n",
        "    threshold = 0.005  #percentatge de marge que deixam a la redistribució\n",
        "    Ntarget = self.len\n",
        "    classes = self.__getlabels__() #indexos de cada clase\n",
        "    afegir = np.array([0,0,0,0,0,0],dtype=np.int64) # de nv mai haurem d'afegir\n",
        "\n",
        "    nmel = len(classes[1]) #nombre inicial de cada clase\n",
        "    nbkl = len(classes[2])\n",
        "    nbcc = len(classes[3])\n",
        "    nakiec = len(classes[4])\n",
        "    nvasc = len(classes[5])\n",
        "    ndf = len(classes[6])\n",
        "\n",
        "    while True:\n",
        "\n",
        "      suma_actual = afegir.sum()\n",
        "\n",
        "      operacio = percentages[0]*Ntarget - nmel\n",
        "      afegir[0] +=  operacio if operacio > 0 else 0\n",
        "      nmel += operacio if operacio > 0 else 0\n",
        "\n",
        "      operacio = percentages[1]*Ntarget - nbkl\n",
        "      afegir[1] += operacio if operacio > 0 else 0\n",
        "      nbkl += operacio if operacio > 0 else 0\n",
        "\n",
        "      operacio = percentages[2]*Ntarget - nbcc\n",
        "      afegir[2] += operacio if operacio > 0 else 0\n",
        "      nbcc += operacio if operacio > 0 else 0\n",
        "\n",
        "      operacio = percentages[3]*Ntarget - nakiec\n",
        "      afegir[3] += operacio if operacio > 0 else 0\n",
        "      nakiec += operacio if operacio > 0 else 0\n",
        "\n",
        "      operacio = percentages[4]*Ntarget - nvasc\n",
        "      afegir[4] += operacio if operacio > 0 else 0\n",
        "      nvasc += operacio if operacio > 0 else 0\n",
        "\n",
        "      operacio = percentages[5]*Ntarget - ndf\n",
        "      afegir[5] += operacio if operacio > 0 else 0\n",
        "      ndf += operacio if operacio > 0 else 0\n",
        "\n",
        "      if (afegir.sum()-suma_actual) < Ntarget*threshold:\n",
        "        break\n",
        "\n",
        "      Ntarget += (afegir.sum()-suma_actual)\n",
        "\n",
        "    #Quedaria afegir a les imatges les còpies\n",
        "    for i in range(len(afegir)):\n",
        "      for j in range(afegir[i]):\n",
        "\n",
        "          self.__addPath__(self.paths[classes[i+1][random.randint(0, len(classes[i+1]) - 1)]])\n",
        "          self.__addlabel__(i+1)\n",
        "\n",
        "    self.len = len(self.labels)\n",
        "\n",
        "  def __setgreyscale__(self,mode):\n",
        "    self.greyscale = mode\n",
        "\n",
        "  def __getpath__(self,index):\n",
        "    return self.paths[index]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      path = self.paths[index]\n",
        "      label = self.labels[index]\n",
        "\n",
        "      image = cv2.imread(path, cv2.IMREAD_GRAYSCALE if self.greyscale else cv2.IMREAD_COLOR)\n",
        "\n",
        "      if SEGMENTATION:\n",
        "        mask = cv2.imread(self.masks[index], cv2.IMREAD_GRAYSCALE)\n",
        "        _, mask_binaria = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
        "        image = cv2.bitwise_and(image, image, mask=mask_binaria)\n",
        "\n",
        "        non_black_pixels = np.where(image > 0)\n",
        "\n",
        "        x_min, x_max = np.min(non_black_pixels[1]), np.max(non_black_pixels[1])\n",
        "        y_min, y_max = np.min(non_black_pixels[0]), np.max(non_black_pixels[0])\n",
        "\n",
        "        marge = 10\n",
        "        if ((x_min - marge) >= 0):\n",
        "            x_min = x_min - marge\n",
        "        if ((x_max + marge) <= 224):\n",
        "            x_max = x_max + marge\n",
        "\n",
        "        cropped_image = image[y_min:y_max, x_min:x_max]\n",
        "        image = cv2.resize(cropped_image, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "      #preprocessament\n",
        "      if CLAHE:\n",
        "        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "        l, a, b = cv2.split(lab)\n",
        "        clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8,8))\n",
        "        l_clahe = clahe.apply(l)\n",
        "        lab_clahe = cv2.merge((l_clahe, a, b))\n",
        "        image = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      if self.greyscale:\n",
        "        image = Image.fromarray(image, mode=\"L\")\n",
        "      else:\n",
        "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "      image = self.transform(image)\n",
        "\n",
        "      return image, label"
      ],
      "metadata": {
        "id": "9kwkcN53v7QT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sets(transformation_training,transformation_default,training_dist,valitation_dist,testing_dist,distribution):\n",
        "\n",
        "  illnes_dictionary = {\n",
        "      'nv': 'Melanocytic nevi',\n",
        "      'mel': 'Melanoma',\n",
        "      'bkl': 'Benign keratosis-like lesions ',\n",
        "      'bcc': 'Basal cell carcinoma',\n",
        "      'akiec': 'Actinic keratoses and intraepithelial carcinoma / Bowens disease',\n",
        "      'vasc': 'Vascular lesions',\n",
        "      'df': 'Dermatofibroma'\n",
        "  }\n",
        "\n",
        "  img_files_1 = sorted(glob('/content/sample_data/HAM10000_images_part_1/*'))\n",
        "  img_files_2 = sorted(glob('/content/sample_data/HAM10000_images_part_2/*'))\n",
        "  img_files = img_files_1 + img_files_2\n",
        "\n",
        "  img_files = np.array(img_files)\n",
        "\n",
        "  mask_files = sorted(glob('/content/sample_data/HAM10000_segmentations_lesion_tschandl/*'))\n",
        "\n",
        "  imgid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in img_files}\n",
        "\n",
        "  mask_path_dict = {os.path.splitext(os.path.basename(x))[0].replace('_segmentation', ''): x for x in mask_files}\n",
        "\n",
        "  #print(mask_path_dict)\n",
        "\n",
        "  #{'ISIC_0024306_segmentation': '/content/sample_data/HAM10000_segmentations_lesion_tschandl/ISIC_0024306_segmentation.png'\n",
        "\n",
        "  metadates['path'] = metadates['image_id'].map(imgid_path_dict.get)\n",
        "  metadates['mask_path'] = metadates['image_id'].map(mask_path_dict.get)\n",
        "\n",
        "  metadates['illness'] = metadates['dx'].map(illnes_dictionary.get)\n",
        "  metadates['illness_code'] = metadates['dx'].map({'nv': 0, 'mel': 1, 'bkl': 2, 'bcc': 3, 'akiec': 4, 'vasc': 5, 'df': 6})\n",
        "\n",
        "  #Aquest illness_code s'utilitzarà com a label de la enfermetat\n",
        "\n",
        "  img_number = len(img_files)\n",
        "\n",
        "  X = metadates.drop('illness_code',axis= 1)\n",
        "  y = metadates['illness_code']\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testing_dist, random_state=42, stratify=y)\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=valitation_dist/(training_dist+valitation_dist), random_state=42, stratify=y_train)\n",
        "\n",
        "  #Una bona idea seria aplicar data augmentation al conjunt d'entrenament ja que les classes estan molt desbalancejades\n",
        "  #Una altra bona idea seria emplear una funció de pèrdua que tengui en compte les classes desbalancejades.\n",
        "\n",
        "  #Antes de guardar els datasets en classes Formes, hauriem de caluclar la mitjana i desviació típica de les imatges\n",
        "  train_data = Formes(X_train['path'].to_numpy(),y_train.to_numpy(),transformation_training)\n",
        "  test_data = Formes(X_test['path'].to_numpy(),y_test.to_numpy(),transformation_default)\n",
        "  validation_data = Formes(X_val['path'].to_numpy(),y_val.to_numpy(),transformation_default)\n",
        "\n",
        "  if SEGMENTATION:\n",
        "\n",
        "    train_data.__setmasks__(X_train['mask_path'].to_numpy())\n",
        "    test_data.__setmasks__(X_test['mask_path'].to_numpy())\n",
        "    validation_data.__setmasks__(X_val['mask_path'].to_numpy())\n",
        "\n",
        "  #prova1: [0.12,0.12,0.06,0.04,0.02,0.02]\n",
        "  #prova2: [0.12,0.12,0.06,0.04,0.02,0.015]\n",
        "  #prova3: [0.13,0.13,0.07,0.05,0.02,0.015]\n",
        "\n",
        "  if distribution != None:\n",
        "    train_data.__redistribute__(distribution)\n",
        "\n",
        "  return train_data, validation_data,test_data"
      ],
      "metadata": {
        "id": "8trqqs38v-Tk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import TextIOWrapper\n",
        "from numpy.linalg import norm\n",
        "import sys\n",
        "import ast\n",
        "import shutil\n",
        "\n",
        "def genera_dists(model,test_data,activation):\n",
        "\n",
        "  PATH = '/content/drive/MyDrive/Features/Embeddings/embeddings_pesos_resnet152multiclass_5.pt.txt'\n",
        "\n",
        "  csv.field_size_limit(sys.maxsize)\n",
        "  dict_illnesses = {0 : 'nv', 1 : 'mel', 2 : 'bkl', 3 : 'bcc', 4 : 'akiec', 5 : 'vasc', 6 : 'df'}\n",
        "\n",
        "  total = test_data.__len__()\n",
        "  DISTANCE = 1\n",
        "\n",
        "  data = [[\"test_img\",\"test_img_label\",\"top_distances\",\"top_labels\",\"bottom_distances\",\"bottom_labels\"]]\n",
        "\n",
        "  ROOT = '/content/drive/MyDrive/Features/Dist/'\n",
        "\n",
        "  current_runs = len([d for d in os.listdir(ROOT) if os.path.isdir(os.path.join(ROOT, d))])\n",
        "  os.makedirs(ROOT+str(current_runs),exist_ok=True)\n",
        "\n",
        "\n",
        "  f = ROOT+str(current_runs)+'/distances.txt'\n",
        "\n",
        "  headers = [\"test_img\",\"test_img_pred_label\",\"test_img_label\",\"top_distances\",\"top_images_labels\",\"top_images_names\",\"bottom_distances\",\"bottom_images_labels\",\"bottom_images_names\"]\n",
        "\n",
        "  with open(f, mode=\"w\", newline=\"\") as file:\n",
        "      writer = csv.writer(file, delimiter=';')\n",
        "      writer.writerow(headers)  # Write header\n",
        "\n",
        "  #iteram imatges de test\n",
        "  for i in range(total):\n",
        "\n",
        "    imatge,pred_label = test_data.__getitem__(i)\n",
        "    actual_label = test_data.__getlabel__(i)\n",
        "\n",
        "    pred_label = dict_illnesses[pred_label]\n",
        "    actual_label = dict_illnesses[actual_label]\n",
        "\n",
        "    imatge = imatge.to(device)\n",
        "    imatge = imatge.unsqueeze(0)\n",
        "\n",
        "    img_path = test_data.__getpath__(i)\n",
        "    img_name = img_path.split('/')\n",
        "    img_name = img_name[len(img_name)-1]\n",
        "\n",
        "    zeros = [0,0,0,0,0]\n",
        "    inf = [np.inf,np.inf,np.inf,np.inf,np.inf]\n",
        "    if DISTANCE == 0:\n",
        "      menor = np.inf\n",
        "      top = np.array(zeros,dtype=object)\n",
        "      bottom = np.array(inf,dtype=object)\n",
        "    else:\n",
        "      menor = 0\n",
        "      top = np.array(inf,dtype=object)\n",
        "      bottom = np.array(zeros,dtype=object)\n",
        "\n",
        "    classes_properes_top = np.array([-1,-1,-1,-1,-1],dtype=object)\n",
        "    classes_properes_bottom = np.array([-1,-1,-1,-1,-1],dtype=object)\n",
        "\n",
        "    paths_top = np.array([\"\",\"\",\"\",\"\",\"\"],dtype=object)\n",
        "    paths_bottom = np.array([\"\",\"\",\"\",\"\",\"\"],dtype=object)\n",
        "    names_top = np.array([\"\",\"\",\"\",\"\",\"\"],dtype=object)\n",
        "    names_bottom = np.array([\"\",\"\",\"\",\"\",\"\"],dtype=object)\n",
        "\n",
        "    output = model(imatge)\n",
        "\n",
        "    my_feature = activation['lastlayer'].flatten().cpu().detach().numpy()\n",
        "\n",
        "    with open(PATH, mode='r') as file:\n",
        "\n",
        "      features = csv.reader(file,delimiter=',')\n",
        "      #botam la capçalera\n",
        "      next(features)\n",
        "\n",
        "      for rows in features:\n",
        "\n",
        "        fcnoutput = np.array(ast.literal_eval(rows[1]))\n",
        "        path = rows[2]\n",
        "        label = rows[3]\n",
        "\n",
        "        comparing_feature = fcnoutput\n",
        "        if DISTANCE == 0:\n",
        "          distance = np.dot(my_feature,comparing_feature)/(norm(my_feature)*norm(comparing_feature))\n",
        "        else:\n",
        "          distance = norm(my_feature - comparing_feature)\n",
        "\n",
        "\n",
        "        #falta corregir pel canvi de calcul de distàncies\n",
        "        if DISTANCE == 0:\n",
        "          for j in range(len(top)):\n",
        "            if distance < menor:\n",
        "              menor = distance\n",
        "            if distance > top[j]:\n",
        "              top[j] = distance\n",
        "              classes_properes_top[j] = dict_illnesses[int(label)]\n",
        "              paths_top[j] = path\n",
        "              names_top[j] = path.split(\"/\")[-1].split(\".\")[0]\n",
        "              break\n",
        "            if distance < bottom[j]:\n",
        "              bottom[j] = distance\n",
        "              classes_properes_bottom[j] = dict_illnesses[int(label)]\n",
        "              paths_bottom[j] = path\n",
        "              names_bottom[j] = path.split(\"/\")[-1].split(\".\")[0]\n",
        "              break\n",
        "        else:\n",
        "          for j in range(len(top)):\n",
        "            if distance > menor:\n",
        "              menor = distance\n",
        "            if distance < top[j]:\n",
        "              top[j] = distance\n",
        "              classes_properes_top[j] = dict_illnesses[int(label)]\n",
        "              paths_top[j] = path\n",
        "              names_top[j] = path.split(\"/\")[-1].split(\".\")[0]\n",
        "              break\n",
        "            if distance > bottom[j]:\n",
        "              bottom[j] = distance\n",
        "              classes_properes_bottom[j] = dict_illnesses[int(label)]\n",
        "              paths_bottom[j] = path\n",
        "              names_bottom[j] = path.split(\"/\")[-1].split(\".\")[0]\n",
        "              break\n",
        "\n",
        "\n",
        "    #carpeta de la nostra imatge\n",
        "    os.makedirs(ROOT+str(current_runs)+'/'+img_name.split('.')[0]+'('+actual_label+')/',exist_ok=True)\n",
        "\n",
        "\n",
        "    if DISTANCE == 0:\n",
        "      sorted_indices = np.argsort(top)[::-1]\n",
        "    else:\n",
        "      sorted_indices = np.argsort(top)\n",
        "\n",
        "    #si utilitzam la distància cosin ens interessa que la distància més gran sigui la primera\n",
        "\n",
        "    top = top[sorted_indices]\n",
        "    classes_properes_top = classes_properes_top[sorted_indices]\n",
        "    paths_top = paths_top[sorted_indices]\n",
        "    names_top = names_top[sorted_indices]\n",
        "\n",
        "    if DISTANCE == 0:\n",
        "      sorted_indices = np.argsort(bottom)[::-1]\n",
        "    else:\n",
        "      sorted_indices = np.argsort(bottom)\n",
        "\n",
        "    bottom = bottom[sorted_indices]\n",
        "    classes_properes_bottom = classes_properes_bottom[sorted_indices]\n",
        "    paths_bottom = paths_bottom[sorted_indices]\n",
        "    names_bottom = names_bottom[sorted_indices]\n",
        "\n",
        "    data_row = [img_name.split('.')[0],pred_label,actual_label,top.astype(float).tolist(),classes_properes_top,names_top,bottom.astype(float).tolist(),classes_properes_bottom,names_bottom]\n",
        "\n",
        "    #a partir d'aqui tendrem les distàncies guardades de menys a mes\n",
        "\n",
        "    os.makedirs('/content/drive/MyDrive/Features/Dist/'+str(current_runs)+'/'+img_name.split('.')[0]+'('+actual_label+')'+'/Top/',exist_ok=True)\n",
        "    os.makedirs('/content/drive/MyDrive/Features/Dist/'+str(current_runs)+'/'+img_name.split('.')[0]+'('+actual_label+')'+'/Bottom/',exist_ok=True)\n",
        "\n",
        "    for k in range(len(paths_top)):\n",
        "      shutil.copy(paths_top[k], '/content/drive/MyDrive/Features/Dist/'+str(current_runs)+'/'+img_name.split('.')[0]+'('+actual_label+')/Top/'+classes_properes_top[k]+'_'+str(k)+'.jpg')\n",
        "\n",
        "    shutil.copy(img_path,'/content/drive/MyDrive/Features/Dist/'+str(current_runs)+'/'+img_name.split('.')[0]+'('+actual_label+')/'+actual_label+'_imatge_test_'+img_name)\n",
        "\n",
        "    for l in range(len(paths_bottom)):\n",
        "      #print('/content/drive/MyDrive/Features/Dist/'+str(current_runs)+'/'+img_name.split('.')[0]+'('+actual_label+')/Bottom/'+classes_properes_bottom[l]+'_'+str(l)+'.jpg')\n",
        "      shutil.copy(paths_bottom[l], '/content/drive/MyDrive/Features/Dist/'+str(current_runs)+'/'+img_name.split('.')[0]+'('+actual_label+')/Bottom/'+classes_properes_bottom[l]+'_'+str(l)+'.jpg')\n",
        "\n",
        "    with open(f, mode=\"a\", newline=\"\") as file:\n",
        "          writer = csv.writer(file, delimiter=';')\n",
        "          writer.writerow(data_row)  # Write row immediately\n",
        "\n",
        "    print(i)\n",
        "    if i == 5:\n",
        "      break\n",
        "\n",
        "  print(\"CSV file written successfully.\")\n",
        "  print(menor)"
      ],
      "metadata": {
        "id": "jh5kfY4tw1BZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = torch.tensor([194.57463374, 139.13953272, 145.36132088]) /255 #rgb\n",
        "std = torch.tensor([35.92275236, 38.90347617, 43.33101831]) / 255\n",
        "\n",
        "TRAINING = 0.80\n",
        "VALIDATION = 0.10\n",
        "TESTING = 0.10\n",
        "SIZE = 224\n",
        "\n",
        "DISTRIBUTIONS = None\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((SIZE,SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = mean, std=std)\n",
        "])\n",
        "\n",
        "transform_training = transforms.Compose([\n",
        "    transforms.Resize((SIZE,SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.3),\n",
        "    transforms.RandomVerticalFlip(p=0.3),\n",
        "    transforms.RandomRotation(degrees=20),\n",
        "    #transforms.RandomResizedCrop(224, scale=(0.8, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = mean, std=std),\n",
        "])\n",
        "\n",
        "CLAHE = False\n",
        "train_data,validation_data,test_data = load_sets(transform_training,transform,TRAINING,VALIDATION,TESTING,DISTRIBUTIONS)\n",
        "\n",
        "PATH = '/content/drive/MyDrive/Runs/Ham10000Resnet152Multiclass/5/pesos_resnet152multiclass_5.pt'\n",
        "\n",
        "resnetmulticlass152 = models.resnet152()\n",
        "num_features = resnetmulticlass152.fc.in_features\n",
        "resnetmulticlass152.fc = nn.Linear(in_features=num_features, out_features=7)\n",
        "\n",
        "resnetmulticlass152.load_state_dict(torch.load(PATH, weights_only=True))\n",
        "\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = input[0].detach()\n",
        "    return hook\n",
        "\n",
        "\n",
        "model = resnetmulticlass152\n",
        "#model.layer4.register_forward_hook(get_activation('layer4'))\n",
        "\n",
        "model.fc.register_forward_hook(get_activation('lastlayer'))\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "genera_dists(model,test_data,activation)"
      ],
      "metadata": {
        "id": "2SkYOuXowGPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "829f11d5-23b5-4160-acd2-9db36b844fc9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "CSV file written successfully.\n",
            "2.8084382038625404\n"
          ]
        }
      ]
    }
  ]
}