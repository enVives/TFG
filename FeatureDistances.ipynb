{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkrHe0i80m0LZKMNFbD3iF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enVives/TFG/blob/main/FeatureDistances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBBrozgPtidt",
        "outputId": "2daa4f5d-a4f2-4c30-f206-e846adb08067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1734, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1734, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import pylab as pl\n",
        "import json\n",
        "\n",
        "from glob import glob\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms,models\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output,display\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "DOWNLOAD = True\n",
        "CLAHE = False\n",
        "SEGMENTATION = True\n",
        "# ORIGEN = '/content/drive/MyDrive/HAM10000/skin-cancer-mnist-ham10000/'\n",
        "\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "if DOWNLOAD:\n",
        "  !rm -rf /content/sample_data/*\n",
        "\n",
        "  !kaggle datasets download -d \"kmader/skin-cancer-mnist-ham10000\"\n",
        "\n",
        "  !unzip -o skin-cancer-mnist-ham10000.zip -d /content/sample_data/\n",
        "\n",
        "  !kaggle datasets download -d \"tschandl/ham10000-lesion-segmentations\"\n",
        "\n",
        "  !unzip -o ham10000-lesion-segmentations.zip -d /content/sample_data/\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadates = pd.read_csv('/content/sample_data/HAM10000_metadata.csv')\n",
        "metadates = metadates.sort_values(by='image_id')"
      ],
      "metadata": {
        "id": "gx-XW9ApuJY7",
        "outputId": "a6b26279-21bc-405b-fc30-4ea93d5858b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sample_data/HAM10000_metadata.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-626631ddb9b8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetadates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/HAM10000_metadata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmetadates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/HAM10000_metadata.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Formes(Dataset):\n",
        "  #Classe on gestionarem les imatges dels fitxers\n",
        "  dict_illnesses = {0 : 'nv', 1 : 'mel', 2 : 'bkl', 3 : 'bcc', 4 : 'akiec', 5 : 'vasc', 6 : 'df'}\n",
        "\n",
        "  def __init__(self, images, labels, transform):\n",
        "        super().__init__()\n",
        "        self.paths = images\n",
        "        self.labels = labels\n",
        "        self.len = len(self.paths)\n",
        "        self.transform = transform\n",
        "        #Per defecte pens que el color pot extreure característiques importants, per tant en primer lloc\n",
        "        #entrenarem les imatges de color\n",
        "        self.greyscale = False\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "  def __setmasks__(self,masks):\n",
        "      self.masks = masks\n",
        "\n",
        "  def __getmask__(self,i):\n",
        "      return self.masks[i]\n",
        "\n",
        "  def __lenmasks__(self):\n",
        "    return len(self.masks)\n",
        "\n",
        "  def __addlabel__(self,label):\n",
        "    self.labels = np.append(self.labels,label)\n",
        "\n",
        "  def __addPath__(self,path):\n",
        "    self.paths = np.append(self.paths,path)\n",
        "\n",
        "  def __getdist__(self):\n",
        "    return pd.Series(self.labels).value_counts()\n",
        "\n",
        "  def __getlabel__(self,i):\n",
        "    return self.labels[i]\n",
        "\n",
        "  def __getlabels__(self):\n",
        "    classes = [[],[],[],[],[],[],[]]\n",
        "    for i in range(len(self.labels)):\n",
        "      classes[self.labels[i]].append(i) #afegim l'index\n",
        "    return classes\n",
        "\n",
        "  def __redistribute__(self,percentages):\n",
        "    #percentages: [15,15,10,10,5,5] percentatges que volem pujar de la resta de classes llevat de nv\n",
        "    threshold = 0.005  #percentatge de marge que deixam a la redistribució\n",
        "    Ntarget = self.len\n",
        "    classes = self.__getlabels__() #indexos de cada clase\n",
        "    afegir = np.array([0,0,0,0,0,0],dtype=np.int64) # de nv mai haurem d'afegir\n",
        "\n",
        "    nmel = len(classes[1]) #nombre inicial de cada clase\n",
        "    nbkl = len(classes[2])\n",
        "    nbcc = len(classes[3])\n",
        "    nakiec = len(classes[4])\n",
        "    nvasc = len(classes[5])\n",
        "    ndf = len(classes[6])\n",
        "\n",
        "    while True:\n",
        "\n",
        "      suma_actual = afegir.sum()\n",
        "\n",
        "      operacio = percentages[0]*Ntarget - nmel\n",
        "      afegir[0] +=  operacio if operacio > 0 else 0\n",
        "      nmel += operacio if operacio > 0 else 0\n",
        "\n",
        "      operacio = percentages[1]*Ntarget - nbkl\n",
        "      afegir[1] += operacio if operacio > 0 else 0\n",
        "      nbkl += operacio if operacio > 0 else 0\n",
        "\n",
        "      operacio = percentages[2]*Ntarget - nbcc\n",
        "      afegir[2] += operacio if operacio > 0 else 0\n",
        "      nbcc += operacio if operacio > 0 else 0\n",
        "\n",
        "      operacio = percentages[3]*Ntarget - nakiec\n",
        "      afegir[3] += operacio if operacio > 0 else 0\n",
        "      nakiec += operacio if operacio > 0 else 0\n",
        "\n",
        "      operacio = percentages[4]*Ntarget - nvasc\n",
        "      afegir[4] += operacio if operacio > 0 else 0\n",
        "      nvasc += operacio if operacio > 0 else 0\n",
        "\n",
        "      operacio = percentages[5]*Ntarget - ndf\n",
        "      afegir[5] += operacio if operacio > 0 else 0\n",
        "      ndf += operacio if operacio > 0 else 0\n",
        "\n",
        "      if (afegir.sum()-suma_actual) < Ntarget*threshold:\n",
        "        break\n",
        "\n",
        "      Ntarget += (afegir.sum()-suma_actual)\n",
        "\n",
        "    #Quedaria afegir a les imatges les còpies\n",
        "    for i in range(len(afegir)):\n",
        "      for j in range(afegir[i]):\n",
        "\n",
        "          self.__addPath__(self.paths[classes[i+1][random.randint(0, len(classes[i+1]) - 1)]])\n",
        "          self.__addlabel__(i+1)\n",
        "\n",
        "    self.len = len(self.labels)\n",
        "\n",
        "  def __setgreyscale__(self,mode):\n",
        "    self.greyscale = mode\n",
        "\n",
        "  def __getpath__(self,index):\n",
        "    return self.paths[index]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      path = self.paths[index]\n",
        "      label = self.labels[index]\n",
        "\n",
        "      image = cv2.imread(path, cv2.IMREAD_GRAYSCALE if self.greyscale else cv2.IMREAD_COLOR)\n",
        "\n",
        "      if SEGMENTATION:\n",
        "        mask = cv2.imread(self.masks[index], cv2.IMREAD_GRAYSCALE)\n",
        "        _, mask_binaria = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
        "        image = cv2.bitwise_and(image, image, mask=mask_binaria)\n",
        "\n",
        "        non_black_pixels = np.where(image > 0)\n",
        "\n",
        "        x_min, x_max = np.min(non_black_pixels[1]), np.max(non_black_pixels[1])\n",
        "        y_min, y_max = np.min(non_black_pixels[0]), np.max(non_black_pixels[0])\n",
        "\n",
        "        marge = 10\n",
        "        if ((x_min - marge) >= 0):\n",
        "            x_min = x_min - marge\n",
        "        if ((x_max + marge) <= 224):\n",
        "            x_max = x_max + marge\n",
        "\n",
        "        cropped_image = image[y_min:y_max, x_min:x_max]\n",
        "        image = cv2.resize(cropped_image, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "      #preprocessament\n",
        "      if CLAHE:\n",
        "        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "        l, a, b = cv2.split(lab)\n",
        "        clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8,8))\n",
        "        l_clahe = clahe.apply(l)\n",
        "        lab_clahe = cv2.merge((l_clahe, a, b))\n",
        "        image = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      if self.greyscale:\n",
        "        image = Image.fromarray(image, mode=\"L\")\n",
        "      else:\n",
        "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "      image = self.transform(image)\n",
        "\n",
        "      return image, label"
      ],
      "metadata": {
        "id": "9kwkcN53v7QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sets(transformation_training,transformation_default,training_dist,valitation_dist,testing_dist,distribution):\n",
        "\n",
        "  illnes_dictionary = {\n",
        "      'nv': 'Melanocytic nevi',\n",
        "      'mel': 'Melanoma',\n",
        "      'bkl': 'Benign keratosis-like lesions ',\n",
        "      'bcc': 'Basal cell carcinoma',\n",
        "      'akiec': 'Actinic keratoses and intraepithelial carcinoma / Bowens disease',\n",
        "      'vasc': 'Vascular lesions',\n",
        "      'df': 'Dermatofibroma'\n",
        "  }\n",
        "\n",
        "  img_files_1 = sorted(glob('/content/sample_data/HAM10000_images_part_1/*'))\n",
        "  img_files_2 = sorted(glob('/content/sample_data/HAM10000_images_part_2/*'))\n",
        "  img_files = img_files_1 + img_files_2\n",
        "\n",
        "  img_files = np.array(img_files)\n",
        "\n",
        "  mask_files = sorted(glob('/content/sample_data/HAM10000_segmentations_lesion_tschandl/*'))\n",
        "\n",
        "  imgid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in img_files}\n",
        "\n",
        "  mask_path_dict = {os.path.splitext(os.path.basename(x))[0].replace('_segmentation', ''): x for x in mask_files}\n",
        "\n",
        "  #print(mask_path_dict)\n",
        "\n",
        "  #{'ISIC_0024306_segmentation': '/content/sample_data/HAM10000_segmentations_lesion_tschandl/ISIC_0024306_segmentation.png'\n",
        "\n",
        "  metadates['path'] = metadates['image_id'].map(imgid_path_dict.get)\n",
        "  metadates['mask_path'] = metadates['image_id'].map(mask_path_dict.get)\n",
        "\n",
        "  metadates['illness'] = metadates['dx'].map(illnes_dictionary.get)\n",
        "  metadates['illness_code'] = metadates['dx'].map({'nv': 0, 'mel': 1, 'bkl': 2, 'bcc': 3, 'akiec': 4, 'vasc': 5, 'df': 6})\n",
        "\n",
        "  #Aquest illness_code s'utilitzarà com a label de la enfermetat\n",
        "\n",
        "  img_number = len(img_files)\n",
        "\n",
        "  X = metadates.drop('illness_code',axis= 1)\n",
        "  y = metadates['illness_code']\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testing_dist, random_state=42, stratify=y)\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=valitation_dist/(training_dist+valitation_dist), random_state=42, stratify=y_train)\n",
        "\n",
        "  #Una bona idea seria aplicar data augmentation al conjunt d'entrenament ja que les classes estan molt desbalancejades\n",
        "  #Una altra bona idea seria emplear una funció de pèrdua que tengui en compte les classes desbalancejades.\n",
        "\n",
        "  #Antes de guardar els datasets en classes Formes, hauriem de caluclar la mitjana i desviació típica de les imatges\n",
        "  train_data = Formes(X_train['path'].to_numpy(),y_train.to_numpy(),transformation_training)\n",
        "  test_data = Formes(X_test['path'].to_numpy(),y_test.to_numpy(),transformation_default)\n",
        "  validation_data = Formes(X_val['path'].to_numpy(),y_val.to_numpy(),transformation_default)\n",
        "\n",
        "  if SEGMENTATION:\n",
        "\n",
        "    train_data.__setmasks__(X_train['mask_path'].to_numpy())\n",
        "    test_data.__setmasks__(X_test['mask_path'].to_numpy())\n",
        "    validation_data.__setmasks__(X_val['mask_path'].to_numpy())\n",
        "\n",
        "  #prova1: [0.12,0.12,0.06,0.04,0.02,0.02]\n",
        "  #prova2: [0.12,0.12,0.06,0.04,0.02,0.015]\n",
        "  #prova3: [0.13,0.13,0.07,0.05,0.02,0.015]\n",
        "\n",
        "  if distribution != None:\n",
        "    train_data.__redistribute__(distribution)\n",
        "\n",
        "  return train_data, validation_data,test_data"
      ],
      "metadata": {
        "id": "8trqqs38v-Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import norm\n",
        "import sys\n",
        "import ast\n",
        "import shutil\n",
        "\n",
        "def genera_dists(model,test_data,activation):\n",
        "\n",
        "  PATH = '/content/drive/MyDrive/Features/Embeddings/embeddings_pesos_resnet152multiclass_5.pt.txt'\n",
        "\n",
        "  csv.field_size_limit(sys.maxsize)\n",
        "  dict_illnesses = {0 : 'nv', 1 : 'mel', 2 : 'bkl', 3 : 'bcc', 4 : 'akiec', 5 : 'vasc', 6 : 'df'}\n",
        "\n",
        "  total = test_data.__len__()\n",
        "\n",
        "  data = [[\"test_img\",\"test_img_label\",\"distances\",\"labels\"]]\n",
        "\n",
        "  current_runs = len([d for d in os.listdir('/content/drive/MyDrive/Features/Dist/') if os.path.isdir(os.path.join('/content/drive/MyDrive/Features/Dist/', d))])\n",
        "  os.makedirs('/content/drive/MyDrive/Features/Dist/'+str(current_runs),exist_ok=True)\n",
        "\n",
        "  f = '/content/drive/MyDrive/Features/Dist/'+str(current_runs)+'/distances.txt'\n",
        "\n",
        "  headers = [\"test_img\",\"test_img_path\",\"test_img_pred_label\",\"test_img_label\",\"distances\",\"labels\",\"paths\"]\n",
        "\n",
        "  with open(f, mode=\"w\", newline=\"\") as file:\n",
        "      writer = csv.writer(file, delimiter=';')\n",
        "      writer.writerow(headers)  # Write header\n",
        "\n",
        "  #iteram imatges de test\n",
        "  for i in range(total):\n",
        "\n",
        "    imatge,pred_label = test_data.__getitem__(i)\n",
        "    actual_label = test_data.__getlabel__(i)\n",
        "\n",
        "    pred_label = dict_illnesses[pred_label]\n",
        "    actual_label = dict_illnesses[actual_label]\n",
        "\n",
        "    imatge = imatge.to(device)\n",
        "    imatge = imatge.unsqueeze(0)\n",
        "\n",
        "    img_path = test_data.__getpath__(i)\n",
        "    img_name = img_path.split('/')\n",
        "    img_name = img_name[len(img_name)-1]\n",
        "\n",
        "    zeros = [0,0,0,0,0]\n",
        "    inf = [np.inf,np.inf,np.inf,np.inf,np.inf]\n",
        "    distances = np.array(zeros,dtype=object)\n",
        "    classes_properes = [-1,-1,-1,-1,-1]\n",
        "    paths = np.array([\"\",\"\",\"\",\"\",\"\"],dtype=object)\n",
        "\n",
        "    output = model(imatge)\n",
        "\n",
        "    my_feature = activation['lastlayer'].flatten().cpu().detach().numpy()\n",
        "\n",
        "    with open(PATH, mode='r') as file:\n",
        "\n",
        "      features = csv.reader(file,delimiter=',')\n",
        "      #botam la capçalera\n",
        "      next(features)\n",
        "\n",
        "      for rows in features:\n",
        "\n",
        "        fcnoutput = np.array(ast.literal_eval(rows[1]))\n",
        "        path = rows[2]\n",
        "        label = rows[3]\n",
        "\n",
        "        comparing_feature = fcnoutput\n",
        "        distance = np.dot(my_feature,comparing_feature)/(norm(my_feature)*norm(comparing_feature))\n",
        "        #distance = norm(my_feature - comparing_feature)\n",
        "\n",
        "        for j in range(len(distances)):\n",
        "          if distance > distances[j]:\n",
        "            distances[j] = distance\n",
        "            classes_properes[j] = dict_illnesses[int(label)]\n",
        "            paths[j] = path\n",
        "            break\n",
        "\n",
        "\n",
        "    data_row = [img_name,pred_label,actual_label,distances,classes_properes]\n",
        "    os.makedirs('/content/drive/MyDrive/Features/Dist/'+str(current_runs)+'/'+img_name.split('.')[0]+'/',exist_ok=True)\n",
        "\n",
        "\n",
        "    sorted_indices = np.argsort(distances)\n",
        "    distances = distances[sorted_indices]\n",
        "    classes_properes = classes_properes[sorted_indices]\n",
        "    paths = paths[sorted_indices]\n",
        "\n",
        "\n",
        "    os.makedirs('/content/drive/MyDrive/Features/Dist/'+str(current_runs)+'/'+img_name.split('.')[0]+'/Top/',exist_ok=True)\n",
        "    os.makedirs('/content/drive/MyDrive/Features/Dist/'+str(current_runs)+'/'+img_name.split('.')[0]+'/Bottom/',exist_ok=True)\n",
        "\n",
        "    for k in range(len(paths)):\n",
        "      shutil.copy(paths[k], '/content/drive/MyDrive/Features/Dist/'+str(current_runs)+'/'+img_name+'('+label+')/Top/'+classes_properes[k]+'_'+str(k)+'.jpg')\n",
        "\n",
        "    shutil.copy(img_path,'/content/drive/MyDrive/Features/Dist/'+str(current_runs)+'/'+img_name+'/'+actual_label+'_imatge_test_'+img_name+'.jpg')\n",
        "\n",
        "\n",
        "    with open(f, mode=\"a\", newline=\"\") as file:\n",
        "          writer = csv.writer(file, delimiter=';')\n",
        "          writer.writerow(data_row)  # Write row immediately\n",
        "\n",
        "    print(i)\n",
        "    if i == 3:\n",
        "      break\n",
        "\n",
        "  print(\"CSV file written successfully.\")"
      ],
      "metadata": {
        "id": "jh5kfY4tw1BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = torch.tensor([194.57463374, 139.13953272, 145.36132088]) /255 #rgb\n",
        "std = torch.tensor([35.92275236, 38.90347617, 43.33101831]) / 255\n",
        "\n",
        "TRAINING = 0.80\n",
        "VALIDATION = 0.10\n",
        "TESTING = 0.10\n",
        "SIZE = 224\n",
        "\n",
        "DISTRIBUTIONS = None\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((SIZE,SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = mean, std=std)\n",
        "])\n",
        "\n",
        "transform_training = transforms.Compose([\n",
        "    transforms.Resize((SIZE,SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.3),\n",
        "    transforms.RandomVerticalFlip(p=0.3),\n",
        "    transforms.RandomRotation(degrees=20),\n",
        "    #transforms.RandomResizedCrop(224, scale=(0.8, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = mean, std=std),\n",
        "])\n",
        "\n",
        "CLAHE = False\n",
        "train_data,validation_data,test_data = load_sets(transform_training,transform,TRAINING,VALIDATION,TESTING,DISTRIBUTIONS)\n",
        "\n",
        "PATH = '/content/drive/MyDrive/Runs/Ham10000Resnet152Multiclass/5/pesos_resnet152multiclass_5.pt'\n",
        "\n",
        "resnetmulticlass152 = models.resnet152()\n",
        "num_features = resnetmulticlass152.fc.in_features\n",
        "resnetmulticlass152.fc = nn.Linear(in_features=num_features, out_features=7)\n",
        "\n",
        "resnetmulticlass152.load_state_dict(torch.load(PATH, weights_only=True))\n",
        "\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = input[0].detach()\n",
        "    return hook\n",
        "\n",
        "\n",
        "model = resnetmulticlass152\n",
        "#model.layer4.register_forward_hook(get_activation('layer4'))\n",
        "\n",
        "model.fc.register_forward_hook(get_activation('lastlayer'))\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "genera_dists(model,test_data,activation)"
      ],
      "metadata": {
        "id": "2SkYOuXowGPY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}